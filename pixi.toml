[workspace]
authors = ["yharby <me@youssefharby.com>"]
channels = ["conda-forge"]
name = "langflow-pixi"
platforms = ["osx-arm64", "linux-64", "win-64", "linux-aarch64"]
version = "0.1.0"

[dependencies]

# Ollama environment - lightweight, just for running Ollama server
[environments]
ollama = ["ollama"]
langflow = ["langflow"]
olmocr = ["olmocr"]

[feature.ollama.dependencies]
ollama = ">=0.12.11,<0.13"

[feature.ollama.tasks]
# All Ollama data (models, configs) stored locally in .ollama/ directory
serve = "OLLAMA_MODELS=.ollama/models OLLAMA_HOST=localhost:11434 OLLAMA_ORIGINS='*' ollama serve"
pull-embedding = "OLLAMA_MODELS=.ollama/models ollama pull jeffh/intfloat-multilingual-e5-large-instruct:f32"
pull-reranker = "OLLAMA_MODELS=.ollama/models ollama pull xitao/bge-reranker-v2-m3"
test = "curl -s http://localhost:11434/api/tags"
test-embedding = """curl -s http://localhost:11434/api/embeddings -d '{
  "model": "jeffh/intfloat-multilingual-e5-large-instruct:f16",
  "prompt": "Instruct: Retrieve semantically similar text\\nQuery: Hello world"
}'"""
list-models = "OLLAMA_MODELS=.ollama/models ollama list"

# Langflow environment - Python-based with all features (fully local)
[feature.langflow.dependencies]
python = ">=3.12.0,<3.13"
pip = ">=25.2,<26"
setuptools = ">=78,<79"
compilers = ">=1.11.0,<1.12.0"
pnpm = ">=10.22.0,<11"
npx = ">=0.1.1,<0.2"
nodejs = ">=22.19.0,<24.10"
uv = ">=0.9.10,<0.10"
numpy = ">=2.2.0,<2.3.0"

[feature.langflow.pypi-dependencies]
# Using SQLite instead of PostgreSQL for fully local setup
langchain-docling = ">=1.1.0, <3"
rapidocr-onnxruntime = ">=1.4.4, <2"

# Platform-specific dependencies - langflow-nightly only for non-Windows platforms
[feature.langflow.target.osx-arm64.pypi-dependencies]
langflow-nightly = { version = ">=1.7.0.dev12"}

[feature.langflow.target.linux-64.pypi-dependencies]
langflow-nightly = { version = ">=1.7.0.dev12"}

[feature.langflow.target.linux-aarch64.pypi-dependencies]
langflow-nightly = { version = ">=1.7.0.dev12"}

[feature.langflow.target.win-64.pypi-dependencies]
langflow-nightly = { version = ">=1.6.5.dev4"}

[feature.langflow.tasks]
run = "langflow run --env-file .env"
run-dev = "langflow run --env-file .env --reload"
run-custom = "langflow run --env-file .env --host 0.0.0.0 --port 7860"

# olmOCR environment - PDF OCR with VLM (cross-platform with GPU/CPU detection)
[feature.olmocr.dependencies]
python = ">=3.12,<3.13"

[feature.olmocr.pypi-dependencies]
olmocr = ">=0.4.6, <0.5"

[feature.olmocr.tasks]
# Run PDF conversion with auto device detection
convert = "python scripts/olmocr/convert_pdfs.py"
# Detect available devices (CUDA/MPS/CPU)
detect = "python scripts/olmocr/convert_pdfs.py --detect-only"
# Install GPU extras for local inference (CUDA only)
install-gpu = "python scripts/olmocr/convert_pdfs.py --install-gpu"
# Convert specific PDF files
convert-files = "python scripts/olmocr/convert_pdfs.py"

[feature.olmocr.target.linux-64.dependencies]
poppler = ">=25.7.0,<26"
qpdf = ">=12.2.0,<13"

[feature.olmocr.target.linux-aarch64.dependencies]
poppler = ">=25.7.0,<26"
qpdf = ">=12.2.0,<13"

[feature.olmocr.target.osx-arm64.dependencies]
qpdf = ">=12.2.0,<13"

[feature.olmocr.target.win-64.dependencies]
qpdf = ">=12.2.0,<13"



# Global tasks that work across environments
[tasks]
install-all = { depends-on = ["install-ollama", "install-langflow"] }
